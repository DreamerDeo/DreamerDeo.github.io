<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Longxu Dou @ Sea AI Lab </title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="./images/sea.png"/>
</head>

<body>
<table style="width:100%;max-width:860px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:center">
                            <name>Longxu Dou 窦隆绪</name>
                        </p>
                        <p>
                            I am a Research Scientist at <a href="https://github.com/sail-sg">Sea AI Lab</a>, working on
                            Natural Language Processing,
                            particularly in multilingual LLM pre-training (<a
                                href="https://sea-sailor.github.io/blog/sailor1/">Sailor</a>/<a
                                href="https://sea-sailor.github.io/blog/sailor2/">Sailor2</a>).

                        </p>
                        <p>
                            I earned my Ph.D. and Bachelor's degree in Computer Science from Harbin Institute of
                            Technology advised by Professor Wanxiang Che.
                            I worked as a research intern at Microsoft Research Asia with Dr.Jian-Guang Lou and at
                            National University of Singapore with Professor Min-Yen Kan.
                        </p>


                        <p style="color: red; font-weight: bold;">
<!--                            We are actively hiring (Senior) Research Scientists, Engineers, and Interns in NLP/LLM.-->
                            Internship positions are available both onsite (in Mainland China, Hong Kong, and Singapore) and remotely.
                            I’m always open to discussions and collaborations.
                            Feel free to reach out via email with your background and interests!
                        </p>

                        <p style="text-align:center">
                            <a href="mailto:doulx@sea.com">Email</a> &nbsp/&nbsp
                            <a href="https://scholar.google.com/citations?user=flgPmvkAAAAJ&hl=en">Google Scholar</a>
                            &nbsp/&nbsp
                            <a href="https://www.linkedin.com/in/longxu-dou-6b167410a/">LinkedIn</a> &nbsp/&nbsp
                            <a href="https://github.com/longxudou/">Github</a> &nbsp/&nbsp
                            <a href="https://x.com/LongxuDou">Twitter</a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="images/longxu-pic3.jpg"><img style="width:80%;max-width:80%;border-radius:50%;"
                                                              alt="profile photo" src="images/longxu-pic3.jpg"
                                                              class="hoverZoomLink"></a>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Recent Research Projects</heading>
                        (# indicates mentorship, * indicates equal contribution.)

                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/sailor2_chat_perf.jpeg' width="200" mar>
                        </div>
                    </td>

                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://sea-sailor.github.io/blog/sailor2/">
                            <papertitle>Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs
                            </papertitle>
                        </a>
                        <br>
                        <strong>Longxu Dou*</strong>, Qian Liu*, Fan Zhou*, Changyu Chen*, Zili Wang, Ziqi Jin, Zichen
                        Liu, Tongyao Zhu, Cunxiao Du, Penghui Yang, Haonan Wang, Jiaheng Liu, Yongchi Zhao, Xiachong
                        Feng, Xin Mao, Man Tsung Yeung,
                        Kunat Pipatanakul, Fajri Koto, Min Si Thu, Hynek Kydlíček, Zeyi Liu, Qunshu Lin, Sittipong
                        Sripaisarnmongkol, Kridtaphad Sae-Khow, Nirattisai Thongchim, Taechawat Konkaew, Narong
                        Borijindargoon, Anh Dao, Matichon Maneegard, Phakphum Artkaew, Zheng-Xin Yong, Quan Nguyen,
                        Wannaphong Phatthiyaphaibun, Hoang H. Tran, Mike Zhang, Shiqi Chen,
                        Tianyu Pang, Chao Du, Xinyi Wan, Wei Lu, Min Lin
                        <!--              Sailor2 Team-->
                        <br>
                        <em>Report</em>, 2025
                        <a href="https://github.com/sail-sg/sailor2">
                            <img src="https://img.shields.io/github/stars/sail-sg/sailor2?style=social"
                                 style="vertical-align: middle;"/>
                        </a>
                        <br>
                        <p>
                            Sailor2 is a community-driven project delivering state-of-the-art multilingual language
                            models in three scales - 1B, 8B, and 20B parameters. Released under the Apache 2.0 license,
                            these models specialize in South-East Asian (SEA) languages, making advanced models more
                            accessible across the region.
                            Building upon the foundation of Qwen2.5 , Sailor2 is continually pre-trained over 500B
                            high-quality tokens to support 15 languages, including English, Chinese, Burmese, Cebuano,
                            Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese,
                            Waray.
                        </p>
                        <font color="red"><strong>Sailor2-20B-Chat achieves a nearly 50% win rate against GPT-4o-0806 on
                            SeaWildBench, showcasing GPT-4o-level performance in local chat scenarios on South-East
                            Asian Languages.</strong></font>
                    </td>
                </tr>


                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/sailor1_pipeline.png' width="200" mar>
                            </div>
                        </td>

                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <a href="https://arxiv.org/pdf/2404.03608">
                                <papertitle>Sailor: Open Language Models for South-East Asia</papertitle>
                            </a>
                            <br>
                            <strong>Longxu Dou*</strong>, Qian Liu*, Guangtao Zeng, Jia Guo, Jiahui Zhou, Xin Mao, Ziqi
                            Jin, Wei Lu, Min Lin
                            <br>
                            <em>EMNLP Demo</em>, 2024
                            <a href="https://github.com/sail-sg/sailor-llm">
                                <img src="https://img.shields.io/github/stars/sail-sg/sailor-llm?style=social"
                                     style="vertical-align: middle;"/></a>
                            <br>
                            <p>
                                Sailor is a family of open language models ranging from 0.5B to 14B parameters, tailored
                                for South-East Asian (SEA) languages. These models are continually pre-trained from
                                Qwen1.5, a great language model for multilingual use cases. From Qwen1.5, Sailor models
                                accept 200B to 400B tokens, primarily covering the languages of English, Chinese,
                                Vietnamese, Thai, Indonesian, Malay, and Lao. The training leverages several techniques,
                                including BPE dropout for improving the model robustness, aggressive data cleaning and
                                deduplication, and small proxy models to optimize data mixture.
                            </p>
                            <font color="red"><strong>Over 150K downloads since March 2024.</strong></font>
                        </td>
                    </tr>


                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                        <tr>
                            <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                    <img src='images/sailcraft.png' width="200" mar>
                                </div>
                            </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://github.com/sail-sg/sailcraft">
                                    <papertitle>SailCraft: Data Toolkit for Sailor Language Models</papertitle>
                                </a>
                                <br>
                                <strong>Longxu Dou</strong>, Qian Liu
                                <br>
                                <em>Tool</em>, 2024
                                <a href="https://github.com/sail-sg/sailcraft">
                                    <img src="https://img.shields.io/github/stars/sail-sg/sailcraft?style=social"
                                         style="vertical-align: middle;"/></a>
                                <br>

                                <p>
                                    The full data processing script used in developing our Sailor models. The repo
                                    provides an end-to-end data processing pipeline for LLM training.
                                    With this codebase, you can clean your own dataset with:
                                    (1) Get filtered data counts after each processing stage.
                                    (2) Easily configure language-specific cleaning rules (we support Arabic, Bengali,
                                    Catalan, Spanish, Basque, French, Hindi, Portuguese, Urdu, and optimize for English,
                                    Indonesian, Vietnamese, Chinese, Thai, Lao, Malay).
                                    (3) Investigate what data was removed at each processing stage.
                                </p>
                            </td>
                        </tr>


                        <!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
                        <!--        <tr>-->
                        <!--          <td style="padding:20px;width:75%;vertical-align:middle">-->
                        <!--            <a href="https://arxiv.org/pdf/2412.01186">-->
                        <!--              <papertitle>SailCompass: Towards Reproducible and Robust Evaluation for Southeast Asian Languages</papertitle>-->
                        <!--            </a>-->
                        <!--            <br>-->
                        <!--            Jia Guo*, <strong>Longxu Dou*</strong>, Guangtao Zeng, Stanley Kok, Wei Lu, Qian Liu-->
                        <!--            <br>-->
                        <!--            <em>Preprint</em>, 2024-->
                        <!--            <a href="https://github.com/sail-sg/sailcompass">-->
                        <!--<img src="https://img.shields.io/github/stars/sail-sg/sailcompass?style=social" style="vertical-align: middle;" />-->
                        <!--            </a>-->
                        <!--            <br>-->
                        <!--          </td>-->
                        <!--        </tr>-->

                        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                            <tbody>
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <img src='images/iclr_2025_regmix.jpg' width="200" mar>
                                    </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/pdf/2407.01492">
                                        <papertitle>RegMix: Data Mixture as Regression for Language Model
                                            Pre-training
                                        </papertitle>
                                    </a>
                                    <br>
                                    Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, <strong>Longxu
                                    Dou</strong>, Tianyu Pang, Jing Jiang, Min Lin
                                    <br>
                                    <em>ICLR <font color="red"><strong>(Spotlight)</strong></font>
                                    </em>, 2025
                                    <a href="https://github.com/sail-sg/regmix">
                                        <img src="https://img.shields.io/github/stars/sail-sg/regmix?style=social"
                                             style="vertical-align: middle;"/>
                                    </a>
                                    <br>
                                    <p>
                                        The data mixture for large language model pre-training significantly impacts
                                        performance, yet how to determine an effective mixture remains unclear. We
                                        propose RegMix to automatically identify a high-performing data mixture by
                                        formulating it as a regression task. RegMix trains many small models on
                                        diverse data mixtures, uses regression to predict performance of unseen
                                        mixtures, and applies the best predicted mixture to train a large-scale
                                        model with orders of magnitude more compute.
                                    </p>
                                </td>
                            </tr>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                <tbody>
                                <tr>
                                    <td style="padding:20px;width:25%;vertical-align:middle">
                                        <div class="one">
                                            <img src='images/nips2024_scaling_vocab.jpg' width="200" mar>
                                        </div>
                                    </td>
                                    <td style="padding:20px;width:75%;vertical-align:middle">
                                        <a href="https://arxiv.org/pdf/2407.13623">
                                            <papertitle>Scaling Laws with Vocabulary: Larger Models Deserve Larger
                                                Vocabularies
                                            </papertitle>
                                        </a>
                                        <br>
                                        Chaofan Tao, Qian Liu#, <strong>Longxu Dou#</strong>, Niklas Muennighoff,
                                        Zhongwei
                                        Wan, Ping Luo, Min Lin, Ngai Wong
                                        <br>
                                        <em>NeurIPS</em>, 2024
                                        <a href="https://github.com/sail-sg/scaling-with-vocab">
                                            <img src="https://img.shields.io/github/stars/sail-sg/scaling-with-vocab?style=social"
                                                 style="vertical-align: middle;"/>
                                        </a>
                                        <br>
                                        <p>
                                            Research on scaling large language models (LLMs) has primarily focused on
                                            model parameters and training data size, overlooking the role of vocabulary
                                            size. We investigate how vocabulary size impacts LLM scaling laws by
                                            training models ranging from 33M to 3B parameters on up to 500B characters
                                            with various vocabulary configurations. We propose three complementary
                                            approaches for predicting the compute-optimal vocabulary size: IsoFLOPs
                                            analysis, derivative estimation, and parametric fit of the loss function.
                                            Our approaches converge on the conclusion that the optimal vocabulary size
                                            depends on the compute budget, with larger models requiring larger
                                            vocabularies. </p>
                                    </td>
                                </tr>


                                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                    <tbody>
                                    <tr>
                                        <td style="padding:0px">
                                            <br>
                                            <p style="text-align:right;font-size:small;">
                                                Design and source code from <a
                                                    href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.

                                        </td>
                                    </tr>
                                    </tbody>
                                </table>
        </td>
    </tr>
</table>
</body>

</html>
